# Методы, средства и технологии мультимедиа

Выполнила студентка: Агеева А.И. 
Группа: М8О-410Б-21

## Часть первая. Задача классификации.

В качестве метрик в задаче классификации использовались accuracy и f1-score. Процесс обучения описан в ноутбуке [classification.ipynb](classification.ipynb)

В таблице приведены получившиеся accuracy для разных моделей.
<table>
    <thead>
        <tr>
            <th rowspan=2>Алгоритм</th>
            <th colspan=2>Модели из sklearn</th>
            <th colspan=2>Самостоятельная реализация</th>
        </tr>
        <tr>
            <th>бейзлайн</th>
            <th>улучшенный бейзлайн</th>
            <th>бейзлайн</th>
            <th>улучшенный бейзлайн</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td><b><i>KNN</i></b></td>
            <td>0.59</td>
            <td>0.655</td>
            <td>0.65</td>
            <td>0.65</td>
        </tr>
        <tr>
            <td><b><i>Логистическая регрессия</i></b></td>
            <td>0.65</td>
            <td>0.655</td>
            <td>0.62</td>
            <td>0.62</td>
        </tr>
        <tr>
            <td><b><i>Линейная регрессия</i></b></td>
            <td>0.655</td>
            <td>0.655</td>
            <td>0.655</td>
            <td>0.655</td>
        </tr>
        <tr>
            <td><b><i>Решающее дерево</i></b></td>
            <td>0.61</td>
            <td>0.625</td>
            <td>0.59</td>
            <td>0.59</td>
        </tr>
        <tr>
            <td><b><i>Случайный лес</i></b></td>
            <td>0.59</td>
            <td>0.625</td>
            <td>0.665</td>
            <td>0.67</td>
        </tr>
        <tr>
            <td><b><i>Градиентный бустинг</i></b></td>
            <td>0.595</td>
            <td>0.665</td>
            <td>0.59</td>
            <td>0.59</td>
        </tr>
    </tbody>
</table>

### Вывод

Из таблицы видно, что, все модели показывали примерно одинаковые результаты по точности от 59% до 66%. Лучшей точности удалось добиться с помощью самостоятельной реализации Случайного леса и препроцессинга данных.

## Часть вторая. Задача регресии.

В качестве метрик в задаче классификации использовались MSE и R2-score. Процесс обучения описан в ноутбуке [regression.ipynb](regression.ipynb)

В таблице приведены получившиеся R2 для разных моделей.
<table>
    <thead>
        <tr>
            <th rowspan=2>Алгоритм</th>
            <th colspan=2>Модели из sklearn</th>
            <th colspan=2>Самостоятельная реализация</th>
        </tr>
        <tr>
            <th>бейзлайн</th>
            <th>улучшенный бейзлайн</th>
            <th>бейзлайн</th>
            <th>улучшенный бейзлайн</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td><b><i>KNN</i></b></td>
            <td>0.148</td>
            <td>0.807</td>
            <td>0.148</td>
            <td>0.797</td>
        </tr>
        <tr>
            <td><b><i>Логистическая регрессия</i></b></td>
            <td>0.219</td>
            <td>0.801</td>
            <td>0.07</td>
            <td>0.8</td>
        </tr>
        <tr>
            <td><b><i>Линейная регрессия</i></b></td>
            <td>0.783</td>
            <td>0.783</td>
            <td>0.78</td>
            <td>0.783</td>
        </tr>
        <tr>
            <td><b><i>Решающее дерево</i></b></td>
            <td>0.755</td>
            <td>0.865</td>
            <td>0.840</td>
            <td>0.864</td>
        </tr>
        <tr>
            <td><b><i>Случайный лес</i></b></td>
            <td>0.862</td>
            <td>0.877</td>
            <td>0.86</td>
            <td>0.862</td>
        </tr>
        <tr>
            <td><b><i>Градиентный бустинг</i></b></td>
            <td>0.857</td>
            <td>0.874</td>
            <td>0.878</td>
            <td>0.879</td>
        </tr>
    </tbody>
</table>

### Вывод

Модели показывают примерно одинаковые результаты по R2 вне, почти не завися от реализации. Очень большая разницы в точности получилось добиться в моделях KNN и логистической регресии при препроцессинге данных. 

